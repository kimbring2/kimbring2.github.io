# Introduction
The contents of this post are explanation for the code posted in [Jetbot Gazebo Simulation Github Repo](https://github.com/kimbring2/jetbot_gazebo). In this post, methods for executing code will not be raised. That can be found on the Github link.

# Table of Contents

1. [What is Jetbot?](#jetbot_intro)
2. [Soccer game using Jetbot](#soccer_game_jetbot)
3. [URDF file for Jetbot](#jetbot_urdf)
4. [Simulating Jetbot at Rviz](#jetbot_rviz)
5. [Simulating Jetbot at Gazebo](#jetbot_gazebo)
6. [Soccer object into Gazebo](#soccer_object_gazebo)
7. [Connect Gazebo and Python](#gazebo_python)
8. [Object location in Python](#object_location)
9. [Multiplt Jetbot in field](#multi_jetbot)
10. [Physical parameter of Jetbot](#physical_parameter)
11. [Soccer ball detection](#jebot_soccer_ball)
12. [Soccer field for Jetbot](#jebot_soccer_field)
13. [Convert continuous action to discrete](#jetbot_action)
14. [Reinforcement Learning of Jetbot](#jetbot_rl)

# Reference
https://github.com/dusty-nv/jetbot_ros : Jetbot SDF file
https://github.com/CentroEPiaggio/irobotcreate2ros : Gazebo parameter
http://gazebosim.org/tutorials/?tut=ros_urdf : URDF file usage in Gazebo
https://towardsdatascience.com/object-detection-with-less-than-10-lines-of-code-using-python-2d28eebc5b11 : Object dection using cvlib
https://github.com/RoboCup-MSL/MSL-Simulator : Soccer field, ball model
https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc? : Reinforcement Learnig model

Thank you every body!!

<a name="jetbot_intro"></a>
# What is Jetbot?
Recently, the Jetson Nano board is released by NVIDIA. And various robots that using that board is selling on the market. One of them is Jetbot that is a kind of line tracer robot. I purchased it at [Amazon Jetbot Purchasing Link](https://www.amazon.com/gp/product/B07WMZ3KLY/ref=ppx_yo_dt_b_asin_title_o03_s00?ie=UTF8&psc=1). 
Designing a customizing robot is a difficult and time-consuming task. Thus, I choose a Jetbot. 

<img src="/assets/NVIDIA-Jetbot.jpg" width="600">

Furthermore, ROS nodes and Gazebo model of Jetbot is released together. Thus, it became easier to test. 

<a name="soccer_game_jetbot"></a>
# Soccer game using Jetbot
First of all, I plan to use a Jetbot for playing soccer. I can use a Gazebo of ROS, Deep Learning for this project.  

<img src="/assets/soccer_jetbot.jpg" width="600">

Gazebo reduces a inconvenience of having to test a robot in a real environment by simulating robot in virtual environment. And Deep Learning technology can make agent play soccer like a human. My goal in this project is combining these two skill for Jetbot playing a soccer like a real human.

<a name="jetbot_urdf"></a>
# URDF file for Jetbot
The URDF file is a format that describes the appearance, physical characteristics, mounting sensor, etc of a ROS robot. Also, to simulate a robot with Gazebo, that file is needed first .

There is also another format called SDF file that is a more general than URDF. But, that format does not support the simulation in Gazebo yet. Thus, it is unavoidable to change SDF file of Jetbot to URDF format. Usaually, Gazebo does not use URDF files directly. It uses a similar format called XACRO that is very similar to URDF. Changing format from URDF to SDF is very easy. 

File must also contain information about characteristics of sensor such as camera for Gazebo simulation. This is created separately in a file with Gazebo extension and loaded from Xacro file. It makes a XACRO file is not too complicated.

You can download model file of Jetbot at [Jetbot URDF file link](https://github.com/kimbring2/jetbot_gazebo/tree/master/jetbot/jetbot_description/urdf).

<a name="jetbot_rviz"></a>
# Simulating Jetbot at Rviz
After creating the URDF file, you can verify it in ROS Rviz. If there is a joing part that can move like a wheel, you can use the GUI to move it in Rviz.

<img src="/assets/Rviz_Jetbot.gif" width="600">

<a name="jetbot_gazebo"></a>
# Simulating Jetbot at Gazebo
Unlike RviZ, Gazebo work with the physical elements of the actual robot and the surrounding environment. Thus, you can get the same effect as testing the robot in the actual environment.

<img src="/assets/Gazebo_Jetbot.gif" width="600">

<a name="soccer_object_gazebo"></a>
# Impementing soccer object into world
A Jetbot model was created and tested using Rviz and Gazebo. Now you can add soccer fields, goals, and soccer balls around the robot and control the robot by applying torque to each wheel. That kind of testing method is provided in Gazabo without making additional code.

[![Jetbot soccer test1](https://img.youtube.com/vi/X7gDfjsVm-g/0.jpg)](https://youtu.be/X7gDfjsVm-g "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

As you can see in the video, the physical conditions of Gazebo are similar to the actual conditions. Thus, it is not easy to control the robot than pure simulated environmnet.

<a name="gazebo_python"></a>
# Connect Gazebo and Python
The jetbot will receive the camera and other sensor information as input and will use it to adjust the speed of the left and right wheels to play soccer. For this, we need Python code that performs the above steps. ROS can input and output these data using a package called rospy.

[![Jetbot soccer test2](https://img.youtube.com/vi/596kUp5ztOw/0.jpg)](https://youtu.be/596kUp5ztOw "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

As you can see in the video above, you can use Python code to output the camera sensor image and adjust the speed of the left and right wheels.

<a name="object_location"></a>
# Object location in Python
The most important goal in soccer will be to put a soccer ball into the opponent's goal. To do this, we should know the location of the soccer ball and goal in real time.

```
football_pose: 
position: 
  x: 0.0
  y: 0.0
  z: 0.119999999992
  
left_goal_pose: 
position: 
  x: -9.0
  y: 0.0
  z: 0.01
  
right_goal_pose: 
position: 
  x: 9.0
  y: 0.0
  z: 0.01
```

If you know the position of each object in the above way, we could use a method that gives a score when it is close to the soccer ball goal or reset a field.

```
# maybe do some 'wait for service' here
reset_simulation = rospy.ServiceProxy('/gazebo/reset_simulation', Empty)
def state_callback(msg):
    # print("msg.name: " + str(msg.name))
    # msg.name: ['ground_plane', 'field', 'left_goal', 'right_goal', 'football', 'jetbot']

    football_index = (msg.name).index("football")
    left_goal_index = (msg.name).index("left_goal")
    right_goal_index = (msg.name).index("right_goal")
    #print("football_index: " + str(football_index))

    football_pose = (msg.pose)[football_index]
    #print("football_pose.position.x: " + str(football_pose.position.x))

    football_pose_x = football_pose.position.x
    print("football_pose_x: " + str(football_pose_x))

    if (football_pose_x > 5):
        reset_simulation()

    left_goal_pose = (msg.pose)[left_goal_index]
    #print("left_goal_pose: " + str(left_goal_pose))

    right_goal_pose = (msg.pose)[right_goal_index]
    #print("right_goal_pose: " + str(right_goal_pose))

    global x
    global y
    global theta

    x = msg.pose[1].position.x
    y = msg.pose[1].position.y
    rot_q = msg.pose[1].orientation
    (roll, pitch, theta) = euler_from_quaternion([rot_q.x, rot_q.y, rot_q.z, rot_q.w])
```

The above code is part of https://github.com/kimbring2/jetbot_soccer/blob/master/jetbot/jetbot_control/src/main.py. After running Gazebo and controller, give a control signal to move the soccer ball using Jetbot. After that, when the soccer ball goal is reached, the simulation is initialized.

<a name="multi_jetbot"></a>
# Multiplt Jetbot in field
For the simplest soccer game, Jetbot that kicks a soccer ball and Jetbot that prevents it are placed in the field, and an environment is established that can get the image of the camera sensor and control wheel speed of each robot.

[![Jetbot soccer test3](https://img.youtube.com/vi/G6HAJRmqrBs/0.jpg)](https://youtu.be/G6HAJRmqrBs "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

As described above, two Jetbots are placed in the field, one jetbot tries to make a goal, and one jetbot tries to defend the goal. In the case of multiplt agent such as that case, the Self-play Reinforcement Learning looks good for using.

<a name="physical_parameter"></a>
# Physical parameter of Jetbot
A Jetbot consists of a chassis and two wheels connected to it. In order to simulate a real environment with Gazebo, these physical parameters are set in the same way as in the actual case. Since Jetbot's exact gazebo physics parameters is not asked now, I temporarily use the parameters of existing robots called [iRobot Gazebo Repository](https://github.com/CentroEPiaggio/irobotcreate2ros).

```
<gazebo reference="chassis">
  <mu1>0.0</mu1>
  <mu2>0.0</mu2>
  <slip1>1.0</slip1>
  <slip2>1.0</slip2>
  <kp>10000000</kp>
  <kd>1</kd>
  <fdir1>1 0 0</fdir1>
  <minDepth>0.0001</minDepth>
  <maxContacts>1</maxContacts>
</gazebo>
 
 <gazebo reference="left_wheel">
  <mu1>50</mu1>
  <mu2>10</mu2>
  <slip1>0.0</slip1>
  <slip2>0.0</slip2>
  <kp>10000000</kp> 
  <kd>1</kd>
  <fdir1>1 0 0</fdir1>
  <minDepth>0.0001</minDepth>
  <maxContacts>1</maxContacts>
</gazebo>

<gazebo reference="right_wheel">
  <mu1>50</mu1>
  <mu2>10</mu2>
  <slip1>0.0</slip1>
  <slip2>0.0</slip2>
  <kp>10000000</kp> 
  <kd>1</kd>
  <fdir1>1 0 0</fdir1>
  <minDepth>0.0001</minDepth>
  <maxContacts>1</maxContacts>
</gazebo>
```

[![Jetbot soccer test4](https://img.youtube.com/vi/8ffXCZ5wCbc/0.jpg)](https://youtu.be/8ffXCZ5wCbc "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

In previous tests, the proper physical parameters are not found, and the Jetbot moved abnormally slowly. However, after applying the appropriate parameters, the robot is able to confirm the movement similar to the actual situation.

<a name="jebot_soccer_ball"></a>
# Soccer ball detection
To play soccer, Jetbot must first find a soccer ball. Jetbot do it using the camera sensor and object detection Deep Learning library.

```
import cvlib as cv

bbox, label, conf = cv.detect_common_objects(cv_image, confidence=0.05)
for obj in label:
   if obj == "sports ball":
       index = label.index(obj)
       x1 = int(bbox[index][0])
       y1 = int(bbox[index][1])
       x2 = int(bbox[index][2])
       y2 = int(bbox[index][3])
       cv2.rectangle(cv_image, (x1,y1), (x2,y2), (0,255,0), 2)
```

<img src="/assets/Jetbot_Soccer_5.gif" width="600">

The Jetbot camera sensor currently in use is a fixed type. Thus, there is a problem that it is not recognized when the soccer ball is too close. However, it is correctly recognized when the entire soccer ball is on the screen.

<a name="jebot_soccer_field"></a>
# Soccer field for Jetbot
In the case of Jetbot, unlike other soccer-only robots, there is no arm to fix the soccer ball. Therefore, there are restrictions on various actions. So I devise a way to set up walls on all sides.

<img src="/assets/Jetbot_Soccer_Field.png" width="600">

Creating a wall in this way will prevent the soccer ball from leaving too far from robot, even if Jetbot doesn't hole the soccer ball.

<a name="jebot_action"></a>
# Convert continuous action to discrete
The most basic action of Jetbot is the speed of both wheels. However, since this speed value is a continuos value, the range is too wide for direct use. Thus, I combine this value and turned it into the six most used actions in soccer.

```
stop_action = [0, 0]
forward_action = [40, 40]
left_action = [40, -40]
right_action = [-40, 40]
bacward_action = [-40, -40]
kick_action = [100, 100]
robot_action_list = [stop_action, forward_action, left_action, right_action, bacward_action, kick_action]
```

Then, I select the generated discrete type action as random and confirm the action of Jetbot.

```
robot1_action_index = random.randint(0,5)
robot2_action_index = random.randint(0,5)
robot1_action = robot_action_list[robot1_action_index]
robot2_action = robot_action_list[robot2_action_index]
    
pub_vel_left_1.publish(robot1_action[0])
pub_vel_right_1.publish(robot1_action[1])

pub_vel_left_2.publish(robot2_action[0])
pub_vel_right_2.publish(robot2_action[1])
```

<img src="/assets/Jetbot_Soccer_6.gif" width="600">

Now that everything is ready, I can use Deep Reinforcement Learning to get Jetbot to play soccer.

<a name="jetbot_rl"></a>
# Reinforcement Learning of Jetbot
For Reinforcement Learning, network input and output are required. In the case of Jetbot, the input is an image obtained by changing the size of the image taken by the camera to 84x84x3. And the output is an action consisting of a ball kick, stopping, moving forward, moving backward, turning left, turning right.

<img src="/assets/Soccer_Network.png" width="600">

Moreover, it is necessary to define a reward for the training of Reinforcement Learning. At the very beginning, the camera is set to give a reward value of 1 when a soccer ball is detected by the camera.

```
r = 0
for obj in label:
    if obj == "sports ball":
        r = 1
        index = label.index(obj)
```

In the initial stage of Reinforcement Learning, training data is collected while performing random actions so that the agent can have a various experiences. We can call this part as exploration.

[![Jetbot soccer test5](https://img.youtube.com/vi/og1vXFjZdbI/0.jpg)](https://youtu.be/og1vXFjZdbI "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>
