# Introduction
There are currently various attempts to apply Deep Reinforcement to actual robot. The characteristics of Reinforcment Learning require over hundred of the initialization of the environment and many data for Deep Learning making it impossible to train the robot in the actual environment.

To overcome these difficulties, people first create a simulation that looks exactly like the real world. In this simulation, training is advanced for the virtual robot. Since then, many researches try to use the tranined model for an actual robot.

In this post, I'll try to confirm these methods are actually working. The test uses a robot called Jetbot that has a GPU on its main board. And the environment is a soccer that is familiar to many people.

Code for that post can be found on the Github [Jetbot Soccer Github](https://github.com/kimbring2/jetbot_soccer).

All the products used in the tests are inexpensive and can be purchased in Amazon. Thus, you can easily try to reproduce them.

# Table of Contents
1. [What is Jetbot?](#jetbot_intro)
2. [Soccer game using Jetbot](#soccer_game_jetbot)
3. [URDF file for Jetbot](#jetbot_urdf)
4. [Simulating Jetbot at Rviz](#jetbot_rviz)
5. [Simulating Jetbot at Gazebo](#jetbot_gazebo)
6. [Install soccer object at Gazebo](#soccer_object_gazebo)
7. [Connect Gazebo and Python](#gazebo_python)
8. [Get object location in Python](#object_location)
9. [Place multiple Jetbot in field](#multi_jetbot)
10. [Physical parameter of Jetbot](#physical_parameter)
11. [Soccer ball detection](#jebot_soccer_ball)
12. [Customize soccer field for Jetbot](#jebot_soccer_field)
13. [Convert continuous action to discrete](#jetbot_action)
14. [Set real soccer environment](#jetbot_real_soccer)
15. [Object location change in Python](#location_change)
16. [Reinforcement Learning test](#rl_test)
    1. [Network structure](#rl_network_structure)
    2. [Reward setting](#rl_reward_setting)
    3. [Training result](#rl_training_result)
    4. [Changing reward setting for accurate control](#rl_change_reward_setting)
    5. [Training result after revision](#rl_second_training_result)
17. [Real Jetbot test](#real_jetbot_test)
    1. [Controlling original Jetbot](#control_original_jetbot)
    2. [Deep Learning model for Jetbot](#deep_learning_model_jetbot)
    3. [Transfering trained Tensorflow weight to Jetbot](#transfer_tensorflow_model)
18. [Soccer robot design](#soccer_robot_design)
    1. [Soccer robot design(Wheel)](#soccer_robot_design_wheel)
    2. [Soccer robot design(Solenoid)](#soccer_robot_design_solenoid)
    3. [Soccer robot design(Lidar)](#soccer_robot_design_lidar)
    4. [Soccer robot design(Infrared)](#soccer_robot_design_infrared)
    5. [Soccer robot design(Integration)](#soccer_robot_design_integration)
    6. [Soccer robot design(Teleoperation)](#soccer_robot_design_teleoperation)

<a name="jetbot_intro"></a>
# What is Jetbot?
Recently, the Jetson Nano board is released by NVIDIA. And various robots that using that board is selling on the market. One of them is Jetbot that is a kind of line tracer robot. I purchased it at [Amazon Jetbot Purchasing Link](https://www.amazon.com/gp/product/B07WMZ3KLY/ref=ppx_yo_dt_b_asin_title_o03_s00?ie=UTF8&psc=1). 
Designing a customizing robot is a difficult and time-consuming task. Thus, I choose a Jetbot. 

<img src="/assets/NVIDIA-Jetbot.jpg" width="800">

Furthermore, ROS nodes and Gazebo model of Jetbot is released together. Thus, it became easier to install and running. 

[![Jetbot ROS test](https://img.youtube.com/vi/cLF34VXZjp4/maxresdefault.jpg)](https://youtu.be/cLF34VXZjp4 "Jetbot Running ROS - Click to Watch!")
<strong>Click to Watch!</strong>

I install ROS on the purchased jetbot and test the basic operation. Installation ROS is very easy and Jupyter Notebook work environment is also easy. 

However, the 18650 battery of 65mm length and 2600mAH capacity installed on Jetbot can not be purchased in Korea local shop. That's why I try to buy from an overseas online shop. It looks difficult importing battery from another country because of customs clearance. However, I find one battery seller in the secondhand market.

<a name="soccer_game_jetbot"></a>
# Soccer game using Jetbot
First of all, I plan to use a Jetbot for playing soccer. I can use a Gazebo of ROS, Deep Learning for this project.  

<img src="/assets/soccer_jetbot.jpg" width="800">

Gazebo reduces a inconvenience of having to test a robot in a real environment by simulating robot in virtual environment. And Deep Learning technology can make agent play soccer like a human. My goal in this project is combining these two skill for Jetbot playing a soccer like a real human.

<a name="jetbot_urdf"></a>
# URDF file for Jetbot
The URDF file is a format that describes the appearance, physical characteristics, mounting sensor, etc of a ROS robot. Also, to simulate a robot with Gazebo, that file is needed first .

There is also another format called SDF file that is a more general than URDF. But, that format does not support the simulation in Gazebo yet. Thus, it is unavoidable to change SDF file of Jetbot to URDF format. Usaually, Gazebo does not use URDF files directly. It uses a similar format called XACRO that is very similar to URDF. Changing format from URDF to SDF is very easy. 

File must also contain information about characteristics of sensor such as camera for Gazebo simulation. This is created separately in a file with Gazebo extension and loaded from Xacro file. It makes a XACRO file is not too complicated.

You can download model file of Jetbot at [Jetbot URDF file link](https://github.com/kimbring2/jetbot_gazebo/tree/master/jetbot/jetbot_description/urdf).

<a name="jetbot_rviz"></a>
# Simulating Jetbot at Rviz
After creating the URDF file, you can verify it in ROS Rviz. If there is a joing part that can move like a wheel, you can use the GUI to move it in Rviz.

<img src="/assets/Rviz_Jetbot.gif" width="800">

<a name="jetbot_gazebo"></a>
# Simulating Jetbot at Gazebo
Unlike RviZ, Gazebo work with the physical elements of the actual robot and the surrounding environment. Thus, you can get the same effect as testing the robot in the actual environment.

<img src="/assets/Gazebo_Jetbot.gif" width="800">

<a name="soccer_object_gazebo"></a>
# Install soccer object at Gazebo
A Jetbot model was created and tested using Rviz and Gazebo. Now you can add soccer fields, goals, and soccer balls around the robot and control the robot by applying torque to each wheel. That kind of testing method is provided in Gazabo without making additional code.

[![Jetbot soccer test1](https://img.youtube.com/vi/X7gDfjsVm-g/maxresdefault.jpg)](https://youtu.be/X7gDfjsVm-g "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

As you can see in the video, the physical conditions of Gazebo are similar to the actual conditions. Thus, it is not easy to control the robot than pure simulated environmnet.

<a name="gazebo_python"></a>
# Connect Gazebo and Python
The jetbot will receive the camera and other sensor information as input and will use it to adjust the speed of the left and right wheels to play soccer. For this, we need Python code that performs the above steps. ROS can input and output these data using a package called rospy.

[![Jetbot soccer test2](https://img.youtube.com/vi/596kUp5ztOw/maxresdefault.jpg)](https://youtu.be/596kUp5ztOw "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

As you can see in the video above, you can use Python code to output the camera sensor image and adjust the speed of the left and right wheels.

<a name="object_location"></a>
# Get object location in Python
The most important goal in soccer will be to put a soccer ball into the opponent's goal. To do this, we should know the location of the soccer ball and goal in real time.

```
football_pose: 
position: 
  x: 0.0
  y: 0.0
  z: 0.119999999992
  
left_goal_pose: 
position: 
  x: -9.0
  y: 0.0
  z: 0.01
  
right_goal_pose: 
position: 
  x: 9.0
  y: 0.0
  z: 0.01
```

If you know the position of each object in the above way, we could use a method that gives a score when it is close to the soccer ball goal or reset a field.

```
# maybe do some 'wait for service' here
reset_simulation = rospy.ServiceProxy('/gazebo/reset_simulation', Empty)
def state_callback(msg):
    # print("msg.name: " + str(msg.name))
    # msg.name: ['ground_plane', 'field', 'left_goal', 'right_goal', 'football', 'jetbot']

    football_index = (msg.name).index("football")
    left_goal_index = (msg.name).index("left_goal")
    right_goal_index = (msg.name).index("right_goal")
    #print("football_index: " + str(football_index))

    football_pose = (msg.pose)[football_index]
    #print("football_pose.position.x: " + str(football_pose.position.x))

    football_pose_x = football_pose.position.x
    print("football_pose_x: " + str(football_pose_x))

    if (football_pose_x > 5):
        reset_simulation()

    left_goal_pose = (msg.pose)[left_goal_index]
    #print("left_goal_pose: " + str(left_goal_pose))

    right_goal_pose = (msg.pose)[right_goal_index]
    #print("right_goal_pose: " + str(right_goal_pose))
```

The above code is part of https://github.com/kimbring2/jetbot_soccer/blob/master/jetbot/jetbot_control/src/main.py. After running Gazebo and controller, give a control signal to move the soccer ball using Jetbot. After that, when the soccer ball goal is reached, the simulation is initialized.

<a name="multi_jetbot"></a>
# Place multiple Jetbot in field
For the simplest soccer game, Jetbot that kicks a soccer ball and Jetbot that prevents it are placed in the field, and an environment is established that can get the image of the camera sensor and control wheel speed of each robot.

[![Jetbot soccer test3](https://img.youtube.com/vi/G6HAJRmqrBs/maxresdefault.jpg)](https://youtu.be/G6HAJRmqrBs "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

As described above, two Jetbots are placed in the field, one jetbot tries to make a goal, and one jetbot tries to defend the goal. In the case of multiplt agent such as that case, the Self-play Reinforcement Learning looks good for using.

<a name="physical_parameter"></a>
# Physical parameter of Jetbot
A Jetbot consists of a chassis and two wheels connected to it. In order to simulate a real environment with Gazebo, these physical parameters are set in the same way as in the actual case. Since Jetbot's exact gazebo physics parameters is not asked now, I temporarily use the parameters of existing robots called [iRobot Gazebo Repository](https://github.com/CentroEPiaggio/irobotcreate2ros).

```
<gazebo reference="chassis">
  <mu1>0.0</mu1>
  <mu2>0.0</mu2>
  <slip1>1.0</slip1>
  <slip2>1.0</slip2>
  <kp>10000000</kp>
  <kd>1</kd>
  <fdir1>1 0 0</fdir1>
  <minDepth>0.0001</minDepth>
  <maxContacts>1</maxContacts>
</gazebo>
 
 <gazebo reference="left_wheel">
  <mu1>50</mu1>
  <mu2>10</mu2>
  <slip1>0.0</slip1>
  <slip2>0.0</slip2>
  <kp>10000000</kp> 
  <kd>1</kd>
  <fdir1>1 0 0</fdir1>
  <minDepth>0.0001</minDepth>
  <maxContacts>1</maxContacts>
</gazebo>

<gazebo reference="right_wheel">
  <mu1>50</mu1>
  <mu2>10</mu2>
  <slip1>0.0</slip1>
  <slip2>0.0</slip2>
  <kp>10000000</kp> 
  <kd>1</kd>
  <fdir1>1 0 0</fdir1>
  <minDepth>0.0001</minDepth>
  <maxContacts>1</maxContacts>
</gazebo>
```

[![Jetbot soccer test4](https://img.youtube.com/vi/8ffXCZ5wCbc/maxresdefault.jpg)](https://youtu.be/8ffXCZ5wCbc "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

In previous tests, the proper physical parameters are not found, and the Jetbot moved abnormally slowly. However, after applying the appropriate parameters, the robot is able to confirm the movement similar to the actual situation.

<a name="jebot_soccer_ball"></a>
# Soccer ball detection
To play soccer, Jetbot must first find a soccer ball. Jetbot do it using the camera sensor and object detection Deep Learning library.

```
import cvlib as cv

bbox, label, conf = cv.detect_common_objects(cv_image, confidence=0.05)
for obj in label:
   if obj == "sports ball":
       index = label.index(obj)
       x1 = int(bbox[index][0])
       y1 = int(bbox[index][1])
       x2 = int(bbox[index][2])
       y2 = int(bbox[index][3])
       cv2.rectangle(cv_image, (x1,y1), (x2,y2), (0,255,0), 2)
```

<img src="/assets/Jetbot_Soccer_5.gif" width="800">

The Jetbot camera sensor currently in use is a fixed type. Thus, there is a problem that it is not recognized when the soccer ball is too close. However, it is correctly recognized when the entire soccer ball is on the screen.

<a name="jebot_soccer_field"></a>
# Customize soccer field for Jetbot
In the case of Jetbot, unlike other soccer-only robots, there is no arm to fix the soccer ball. Therefore, there are restrictions on various actions. So I devise a way to set up walls on all sides.

<img src="/assets/Jetbot_Soccer_Field.png" width="800">

Creating a wall in this way will prevent the soccer ball from leaving too far from robot, even if Jetbot doesn't hole the soccer ball.

<a name="jebot_action"></a>
# Convert continuous action to discrete
The most basic action of Jetbot is the speed of both wheels. However, since this speed value is a continuos value, the range is too wide for direct use. Thus, I combine this value and turned it into the six most used actions in soccer.

```
stop_action = [0, 0]
forward_action = [40, 40]
left_action = [40, -40]
right_action = [-40, 40]
bacward_action = [-40, -40]
kick_action = [100, 100]
robot_action_list = [stop_action, forward_action, left_action, right_action, bacward_action, kick_action]
```

Then, I select the generated discrete type action as random and confirm the action of Jetbot.

```
robot1_action_index = random.randint(0,5)
robot2_action_index = random.randint(0,5)
robot1_action = robot_action_list[robot1_action_index]
robot2_action = robot_action_list[robot2_action_index]
    
pub_vel_left_1.publish(robot1_action[0])
pub_vel_right_1.publish(robot1_action[1])

pub_vel_left_2.publish(robot2_action[0])
pub_vel_right_2.publish(robot2_action[1])
```

<img src="/assets/Jetbot_Soccer_6.gif" width="800">

Now that everything is ready, we can use Deep Reinforcement Learning for Jetbot playing soccer.

<a name="jetbot_real_soccer"></a>
# Set real soccer environment
After Jetbot can play soccer using Reinforcement Learning in Gazebo. We should transfer the trained model to a real Jetbot and make sure it works well in a real environment.

<img src="/assets/soccer_set.png" width="800">

First, I pick an appropriate goal and a soccer ball on Amazon [Soccer post and ball Amazon link](https://www.amazon.com/gp/product/B07VDN239T/ref=ppx_yo_dt_b_asin_title_o01_s00?ie=UTF8&psc=1). The shape of the simulation are a little different from these things. Thus, shapd of goal and goal post in simulation should be changed. Also, I select green soccer field rug for floor [Soccer field rug Amazon link](https://www.amazon.com/gp/product/B07Q122XQ8/ref=ppx_yo_dt_b_asin_title_o00_s00?ie=UTF8&psc=1).

<a name="location_change"></a>
# Object location change in Python
When performing Reinforcement Learning, position changed of soccer ball when episode is initialized is needed to confirm traning is done correct. In Gazebo, we can change the position of an object in the world using “/ gazebo / set_model_state” topic.

```
from gazebo_msgs.msg import ModelState

set_state = rospy.ServiceProxy('/gazebo/set_model_state', SetModelState)
pose = Pose() 
pose.position.x = np.random.randint(0,6)
pose.position.y = np.random.randint(0,6)
pose.position.z = 0.12
  
pose.orientation.x = 0
pose.orientation.y = 0
pose.orientation.z = 0
pose.orientation.w = 0
    
state_model = ModelState()   
state_model.model_name = "football"
state_model.pose = pose
resp = set_state(state_model)
```

When one episode is completed, the initialization position of the Jetbot is fixed. However, the position of the soccer ball can be changed every time as can be seen in the video, 

<img src="/assets/Jetbot_Soccer_7.gif" width="800">

<a name="rl_test"></a>
# Reinforcement Learning test
For this project, I chose Deep Reinforcement Learning as the control method for Jetbot playing soccer. This method train robot control neural networks via reward.

<a name="rl_network_structure"></a>
## Network Structure
For Reinforcement Learning, network input and output are required. In the case of Jetbot, the input is an image obtained by changing the size of the image taken by the camera to 84x84x3. And the output is an action consisting of a ball kick, stopping, moving forward, moving backward, turning left, turning right.

<img src="/assets/Jetbot_RL_Network.png">

<a name="rl_reward_setting"></a>
## Reward Setting

Moreover, it is necessary to define a reward for the training of Reinforcement Learning. At the very beginning, the camera is set to give a reward value of 1 when a soccer ball is detected by the camera.

```
r = 0
for obj in label:
    if obj == "sports ball":
        r = 1
        index = label.index(obj)
```

In the initial stage of Reinforcement Learning, training data is collected while performing random actions so that the agent can experience various situation. This part is for exploration of Reinforcement Learning.

[![Jetbot soccer test5](http://img.youtube.com/vi/og1vXFjZdbI/maxresdefault.jpg)](https://youtu.be/og1vXFjZdbI "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

After trarning, I can compare the behavior of Jetbot with the previous behavior to check Reinforcement Learning is effective.

<a name="rl_training_result"></a>
## Training result
When Reinforcement Learning works normally, after setting as above, Jetbot must see the direction where the soccer ball is located after a certain episode.I observe the behavior of Jetbot after Reinforcement Learning as a condition to receive a reward of 1 when soccer ball is detected.

[![First Jetbot Reinforcement Learning test1](https://img.youtube.com/vi/bbSEE9zK8a4/maxresdefault.jpg)](https://youtu.be/bbSEE9zK8a4 "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

<a name="rl_change_ball"></a>
## Changing ball size for well detection
I am able to confirm that there is a case where the total sum of reward do not increase even though the behavior of Jetbot is like finding a soccer ball.

<img src="/assets/soccer_ball_range.png" width="800">

Thus, I reconfirm the part that detects the soccer ball. The minimum distance for detection is confirmed around 1.3m. I think soccer ball size is needed bigger than now. 

<img src="/assets/middle_soccer_field.png" width="800">
Reflecting these circumstances, the size of the soccer field has been reduced to 2/3 due to the limited distance of the Jetbot camera. And the size of the soccer ball is made slightly larger so that it can be taken well by the camera.

<a name="rl_change_reward_setting"></a>
## Changing reward setting for accurate control
After adjusting the size of the field and the size of the ball to match Jetbot, the conditions for reward for Reinforcmenet Learning are slightly raised. When soccer ball enter the screen, Jetbot get a reward, but this time ball should be located around the center of screen.

```
r = 0
for obj in label:
  if obj == "sports ball":
    index = label.index(obj)
    ball_bbox_x1 = bbox[index][0]
    ball_bbox_x2 = bbox[index][2]
    ball_bbox_middle = (ball_bbox_x1 + ball_bbox_x2) / 2.0
    #print("ball_bbox_middle: " + str(ball_bbox_middle))
            
    if ( (ball_bbox_middle > 330) & (ball_bbox_middle < 390) ):
      r = 1
```

[![Second Jetbot Reinforcement Learning](https://img.youtube.com/vi/rW7n_T9aAQc/maxresdefault.jpg)](https://youtu.be/rW7n_T9aAQc "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

<a name="rl_second_training_result"></a>
## Training result after revision
After trarning about 12 hours, when I check a behavior of Jetbot, it seems to try to pick up a soccer ball inside the camera angle. Therefore, I can confirm that there is no problem with the Reinforcement Learning algorithm itself. 

However, Jetbot do not take a soccer ball exactly in the middle of the camera. Thus, I check a the conditions that give reward and find it is too wide and decide to narrow the scope.

[![Third Jetbot Reinforcement Learning](https://img.youtube.com/vi/vQseC4pwsBY/maxresdefault.jpg)](https://youtu.be/vQseC4pwsBY "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

I train after reducing the range displayed on the soccer ball camera that gives the reward. The results confirm that Jetbot shows the action to pick up the soccer ball in the middle of the camera. 

However, sometimes there is a phenomenon where Jetbot was standing in a place that was too long. Such a phenomenon is considered to be a phenomenon that occurs because there is no effect on the reward even if only the time has passed without doing anything.

<a name="real_jetbot_test"></a>
# Real Jetbot test
In addition to trarning Reinforcement Learning by simulation, I also test a the actual Jetbot to reduce the difference between simulation and actual.

<a name="control_original_jetbot"></a>
## Controlling original Jetbot
While trarning the simulation, the actual environment is constructed for using the trained model. I use various basic examples of Jetbot to check the operation of the robot in this environment. The carpet for soccer field looks like a little small. It is necessary to use the entire area including the white line.

[![First Jetbot real soccer test](https://img.youtube.com/vi/QxG27A8Z_L0/maxresdefault.jpg)](https://youtu.be/QxG27A8Z_L0
 "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

While creating the actual environment directly, I am able to discover difference between simulation and actual. The biggest one is that Jetbot should not collide with each other because the camera is exposed to the outside. It seems that camera will be broken at the time of collision.

<a name="deep_learning_model_jetbot"></a>
## Deep Learning model for Jetbot
In addition to creating a soccer field for Jetbot, I should also consider how to use the Tensorflow model trained in Gazebo. First of all, it is necessary to implement the input and output of the network of the neural network used in Gazebo as it is in the actual Jetbot. Thus, it will be possible to use the trained network without change.

<img src="/assets/env_change.png" width="800">

In particular, embedded boards such as Jetson Nano may not use common libraries can be used on general research PCs. Thus, this kind of library also needs to be changed. For example, I use cvlib when detecting a soccer ball in Gazebo, but this library correctly can not be installed in Jetson Nano.

```
import jetson.inference
import jetson.utils

b_channel, g_channel, r_channel = cv2.split(cv_image)
alpha_channel = np.ones(b_channel.shape, dtype=b_channel.dtype) * 50 # creating a dummy alpha channel image.
img_BGRA = cv2.merge((b_channel, g_channel, r_channel, alpha_channel))
in_arr = jetson.utils.cudaFromNumpy(img_BGRA)

overlay = "box,labels,conf"

net = jetson.inference.detectNet("ssd-mobilenet-v2", threshold=0.3)
detections = net.Detect(in_arr, width, height, overlay)
```

First, jetson.inference and jetson.utils is needed to be imported in the python file. Next, change a format of camera image from numpy array to tensor using utils.cudaFromNumpy. Overlay parameter should be given to inference.detectNet function for boxing, labeling and getting confidence of detection.

<img src="/assets/jetbot_detect_result.jpg" width="800">

In fact, if you look at the results of a test using Jetbot, you can see that a fairly wide range of objects are detected and displayed with high accuracy.

<a name="transfer_tensorflow_model"></a>
## Transfering trained Tensorflow weight to Jetbot
And considering the limited Jetson Nano board GPU compared to a normal PC, it is necessary to use a trained model to optimize when inference. In the Jetson series, this can be done mainly using TensorRT. First the model is trained and saved using Gazebo and Reinforcmenet Learning on a general PC with GPU. Next, convert the weight of the saved model to TensorRT inference graph format, and use this to execute inference in session.

```
# Import TensorFlow and TensorRT
import tensorflow as tf
import tensorflow.contrib.tensorrt as trt
import numpy as np

# Inference with TF-TRT `MetaGraph` and checkpoint files workflow:
graph = tf.Graph()
with graph.as_default():
    with tf.Session() as sess:
        # First create a `Saver` object (for saving and rebuilding a
        # model) and import your `MetaGraphDef` protocol buffer into it:
        saver = tf.train.import_meta_graph("model-1.cptk.meta")
        # Then restore your training data from checkpoint files:
        
        checkpoint = tf.train.get_checkpoint_state("/home/jetbot/Notebooks/")
        input_checkpoint = checkpoint.model_checkpoint_path
        saver.restore(sess, input_checkpoint)
        
        #for op in graph.get_operations():
        #    print(op.name)
            
        # Finally, freeze the graph:
        your_outputs = ["main/ArgMax"]
        frozen_graph = tf.graph_util.convert_variables_to_constants(
            sess,
            tf.get_default_graph().as_graph_def(),
            output_node_names=["main/ArgMax"])
        
        # Now you can create a TensorRT inference graph from your
        # frozen graph:
        trt_graph = trt.create_inference_graph(
            input_graph_def=frozen_graph,
            outputs=["main/ArgMax"],
            max_batch_size=1,
            max_workspace_size_bytes=50000,
            precision_mode="FP16")
        
        tf.import_graph_def(trt_graph, name='')
    
        x = graph.get_tensor_by_name('main/scalarInput:0')
        y = graph.get_tensor_by_name('main/ArgMax:0')
        trainLength = graph.get_tensor_by_name('main/trainLength:0')
        batch_size = graph.get_tensor_by_name('main/batch_size:0')
    
        input_test = np.empty([84, 84, 3], dtype=float)
        input_test_flatten = input_test.flatten()
        
        y_out = sess.run(y, feed_dict={
            x: [input_test_flatten],
            trainLength: 1,
            batch_size: 1
        })
        
        print(y_out)
```

<a name="soccer_robot_design"></a>
# Soccer robot design
I conclude that no matter how much football fields I changed, Jetbot need to take a soccer ball or kick it. Thus, I decided to design a robot exclusively for soccer and using the Jetson Nano.

<img src="/assets/soccer_robot_design.png" width="300">

<a name="soccer_robot_design_wheel"></a>

Fortunately, 3d model of NVIDIA Kaya robots and robot participating in Robocup are available online. By using both of these as a reference and utilizing 3d printer, I can easily create a Jetbot for football. And recently, the price of 3D printers has dropped sharply. Thus, I manage to find one printer to create a soccer robot at a very affordable price.

1. [Kaya robot model](https://cad.onshape.com/documents/03aa2560e7a40b2b7da40e12/w/001dbb6db63b0092c9ea5823/e/37043abce9062fab02c40889)
2. [Robocup robot model](https://www.stlfinder.com/model/naghshe-jahan-2010-robocup-soccer-robot/2284603/)
3. [3D printer](https://ko.aliexpress.com/item/32829861835.html?spm=a2g0s.9042311.0.0.42964c4dsLAMTk)

<a name="soccer_robot_design_2"></a>

Jetbot's soccer robot design has two primary focuses. Kaya robot's motors, screws, batteries, electronics, etc., initially offered by NVIDIA, are the latest parts sold today and should be used wherever possible. Second, the overall design of the robot should be as similar as possible to the design of the robot that entered Robotbup.

<img src="/assets/soccer_bot_part.png" width="800">

To achieve such a goal, two robot parts are once separated and measured to confirm what parts are needed to be newly designed or modified.

<img src="/assets/soccer_proto_1.png" width="400"> <img src="/assets/soccer_proto_2.png" width="400">

The new robot will have a solenoid in the center for a kicking and the required parts will be produced using a 3D printer.

<a name="soccer_robot_design_wheel"></a>
## Soccer robot design(Wheel)
[![First Ender 3D printer test](https://img.youtube.com/vi/tECJiKpq6ac/0.jpg)](https://youtu.be/tECJiKpq6ac "Jetbot Soccer Play - Click to Watch!") [![Second Ender 3D printer test](https://img.youtube.com/vi/TiPLMTR9AU0/0.jpg)](https://youtu.be/TiPLMTR9AU0 "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

After a little trial and error, I am able to produce a parts designed to fix the Dynamixel in good quality.

<img src="/assets/wheel_suspention_1.jpg" width="400"> <img src="/assets/wheel_suspention_2.jpg" width="400">

After creating a suspention for each wheel, I also produce a the body of the robot for supporting a suspention together.

[![Third Ender 3D printer test](https://img.youtube.com/vi/bR9G6gmx8bE/0.jpg)](https://youtu.be/bR9G6gmx8bE "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

After constructing the first layer of the robot body, I attache four motors to the body with the suspension I created earlier using 3D printer.

[![OmniWheel test](https://img.youtube.com/vi/VNq4rqZAKec/maxresdefault.jpg)](https://youtu.be/VNq4rqZAKec "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

The power supply to the dynamixel is 12V, which utilizes the Jetbot of WaveShares main board which has 3 18560 battery.

<img src="/assets/circuit_design_soccer.png" width="800">

The U2D2 board and U2D2 power hub can be purchased at the Robotis shopping mall. However, if you have existing one, you can change only the 12V power supply method and use the rest as it is.

1. [U2D2](http://www.robotis.us/u2d2/)
2. [U2D2 Power Hub](http://www.robotis.us/u2d2-power-hub-board-set/)

It is judged that the size omniwheel is too large. Thus, I decided to replace it with a slightly smaller wheel. Basically, onmiwheel product which provides a 3D model is selected for using in Gazebo simualtion.

<img src="/assets/soccer_proto_4.png" width="800">

1. [Omniwheel shop(Korea local shop)](http://robomecha.co.kr/product/detail.html?product_no=10&cate_no=1&display_group=2)
2. [Omniwheel 3D model](https://cad.onshape.com/documents/9a91ce8d931df48891a33741/w/d07aae74b658bfdb32b3c1a2/e/55f0a4a11d07b8ae71bce952)

Dynamixel MX-12W is not changed. 

[![Dynamixel test 1](https://img.youtube.com/vi/4q6_ML3Ii8o/0.jpg)](https://youtu.be/4q6_ML3Ii8o "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

ID and communication method and firmware version of of Dynamixel can given via a program provided by ROBOTIS. I test a motor seperately before installing it to robot body. A power supply and TTL communication can be done by using a U2D2 board and power hub.

<img src="/assets/assembly_1.jpg" height="400"> <img src="/assets/assembly_2.jpg" height="400">

After confirming only the operation of the motor separately, the motor and the control, power board are combined to the robot body. 

<img src="/assets/ccw_setting.png" width="400"> <img src="/assets/speed_setting.png" width="400">

In the case of dynamixel, the initial mode is the joint mode. Mode is needed to be changed to the wheel mode for using soccer robot. This can be achieved by setting the CCW Angle Limit of motor to 0. To rotate the motor set in the wheel mode at a specific speed, you just need to give a specific value to Moving Speed.

[![Dynamixel test 2](https://img.youtube.com/vi/3NiTv4gDRWQ/hqdefault.jpg)](https://youtu.be/3NiTv4gDRWQ "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

Next, I test adjusting the speed of dynamixel using rostopic, as in the previous Jetbot.

[![Dynamixel test 3](https://img.youtube.com/vi/VT6AOI11sbs/hqdefault.jpg)](https://youtu.be/VT6AOI11sbs "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

The Jetbot Soccer version uses an omniwheel that has a many sub wheel. In order to properly simulate this with Gazebo, we must make sure that each sub wheel rotates correctly. First, I check it using RViz in the same way as a main wheel.

[![Omniwheel RVIz test](https://img.youtube.com/vi/Oa-rRioxU7M/hqdefault.jpg)](https://youtu.be/Oa-rRioxU7M "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

After completing the test with RVIz, the test is similarly performed with Gazebo. It is confirmed that when the friction with the floor is large, the phenomenon that the sub wheel do not rotate properly is occurred. Finding the optimal friction parameters will be an important task.

[![Omniwheel Gazebo test](https://img.youtube.com/vi/0bvrdl4Z4Lo/hqdefault.jpg)](https://youtu.be/0bvrdl4Z4Lo "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

<a name="soccer_robot_design_solenoid"></a>
## Soccer robot design(Solenoid)
The mechanism for controlling the ball is composed of a rubber roller for fixing and a solenoid electromagnet for kicking.

1. [Engraving rubber roller(Made in Korea)](
https://www.ebay.com/itm/50mm-Engraving-Rubber-Roller-Brayer-Stamping-Printing-Screening-Tool-Korea-/153413802463)

For the part for grabiing the soccer ball, I use a part of engraving roller. The core of the roller is made by 3D printer and connected to a DC motor which is included in origin Jetbot.

[![Roller test 1](https://img.youtube.com/vi/u3jQLBPoC7Q/0.jpg)](https://youtu.be/u3jQLBPoC7Q "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

After completing the operation test with the roller alone, it is mounted on the of the robot second body layer. Since then, I test whether robot can actually hold the ball.

[![Roller test 2](https://img.youtube.com/vi/ve6R_gJHDgg/0.jpg)](https://youtu.be/ve6R_gJHDgg "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

For the part for kicking the soccer ball, I use a solenoid electromagnet and motor driver for easy control ot it. 

[![Solenoid test 1](https://img.youtube.com/vi/m4lTXv2Z0Wg/hqdefault.jpg)](https://youtu.be/m4lTXv2Z0Wg "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

When the solenoid electromagnet is applied a voltage of 12V, iron bar back of middle of motor is pulled. When the power supply is removed after that, the iron bar hit the ball of front by the elastic force of the spring.

2. [Solenoid electromagnet and driver(Made in Korea)](
http://www.dnjmall.com/shop/goods/goods_view.php?&goodsno=1409&category=027)

After testing the operation of a single solenoid motor, it is insalled to the robot body. Then, I perform an test it with Roller, which catches the ball to confirm that robot can do a catch and kick operation simultaneously.

[![Solenoid test 2](https://img.youtube.com/vi/PC2Yblt2Gek/hqdefault.jpg)](https://youtu.be/PC2Yblt2Gek "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

As a result of the test, the robot can hold and kick a ball without any problem in stop situaiton. However, there are two minor problem that robot can not kick a ball far because of ball is little heavy and hold perfectly because of absence of side wall around roller.

<img src="/assets/solenoid_circuit.png" width="800">

To control a solenoid electromagnet through the Jetson Nano, a driver that relays control signals and power between the 12V power supply and the motor is required. I used drivers sold together with a solenoid electromagnet. First, connect the motor and 12V power supply to driver as can be seen in the schematic diagram. Thereafter, the solenoid electromagnet can be controlled with the sixth wire of the CON1 pin group of the driver.

Jetson Nano GPIO control is performed by referring to the https://www.jetsonhacks.com/2019/06/07/jetson-nano-gpio/ blog post.

[![Solenoid test 3](https://img.youtube.com/vi/Ku3qHhWfQaA/hqdefault.jpg)](https://youtu.be/Ku3qHhWfQaA "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

You can control the solenoid electromagnet by setting the GPIO to a value of 0 or 1 by terminal command as you can see it in the video. 

[![Solenoid test 4](https://img.youtube.com/vi/133Yu_ekduQ/hqdefault.jpg)](https://youtu.be/133Yu_ekduQ "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

Finally, to control the solenoid electromagnet in the ROS, we should control GPIO using code. I use a GPIO library of Jetson (https://github.com/NVIDIA/jetson-gpio) provided by NVIDIA.

It is determined that directly connecting the solenoid motor directly to the 12V power supply can not enough force to kick the ball far. Thus, large capacity capacitor and charging circuit for it is added. Thankfully I could find circuit and component for this at https://drive.google.com/file/d/17twkN9F0Dghrc06b_U9Iviq9Si9rsyOG/view.

<img src="/assets/reinforced_solenoid.png" width="800">

1. [Capacitor buying link](https://www.aliexpress.com/item/32866139188.html?spm=a2g0s.9042311.0.0.2db94c4dNsaPDZ)
2. [Capacitor Charger](https://www.aliexpress.com/item/32904490215.html?spm=a2g0s.9042311.0.0.27424c4dANjLyy)
3. [Limit switch(Relay module can replace it)](https://www.aliexpress.com/item/32860423798.html?spm=a2g0s.9042311.0.0.2db94c4dNsaPDZ)

After a 250v 1000uf capacitor and a ±45V-390V capacitor charger are added, a solenoid can push a heavy a billiard ball to considerable distance.

[![Solenoid test 5](https://img.youtube.com/vi/n_TAL5K73aA/hqdefault.jpg)](https://youtu.be/n_TAL5K73aA "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

Because the wheel and roller part have all simple structure, they can be implemented using the default controller of Gazebo. However. the solenoid part need a custom controller since it has spring part. You can find a official tutorial for that at http://gazebosim.org/tutorials?tut=guided_i5&cat=. 

```
namespace gazebo
{
	class SolenoidElectromagnetSpringPlugin : public ModelPlugin
	{
		public: SolenoidElectromagnetSpringPlugin() {}     
...
...
...

public: void OnRosMsg(const std_msgs::Float32ConstPtr &_msg)
{
	std::cout << "_msg->data: " << _msg->data << std::endl;
		 
	for (int i = 0; i < 10; i++)
	    this->joint->SetForce(0, 1000 * float(_msg->data));
}

protected: void OnUpdate()
{
	double current_angle = this->joint->Position(0);
	for (int i = 0; i < 5; i++)
		this->joint->SetForce(0, (this->kx * (this->setPoint - current_angle)));
}
```

After testing the tutorial plugin first, add OnRosMsg, OnUpdate function to ModelPlugin. The OnRosMsg function is for receiving message transmitted to rostopic which include force applied to the solenoid by battery. The OnUpdate function is a part for defining a force applied by the spring.

```
<gazebo>
  <plugin name="stick_solenoid_electromagnet_joint_spring" filename="libsolenoid_electromagnet_joint_spring_plugin.so">
    <kx>1000</kx>
    <set_point>0.01</set_point>
    <joint>stick</joint>
  </plugin>
</gazebo>
```

Adding the above part to jetbot_soccer.gazebo after defining the plugin can make it possible to use a custom pluging in that joint.

<a name="soccer_robot_design_lidar"></a>
## Soccer robot design(Lidar)
Soccer robot need to check a obstacle of front side. Using only camera sensor is not enough for that. Thus, I decide adding lidar sensor.

<img src="/assets/lidar_circuit.png" width="800">

The lidar sensor I use can measure only on the front side. It requires 5V, GND and UART GPIO pins of Jetson Nano.

[![lidar test 1](https://img.youtube.com/vi/2b6BUH5tF1g/sddefault.jpg)](https://youtu.be/2b6BUH5tF1g "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

After checking operation of lidar sensor at simulation. I also check real lidar sensor can measure distance of front obstacle.  

[![lidar test 2](https://img.youtube.com/vi/LNNTYWV4W0Y/sddefault.jpg)](https://youtu.be/LNNTYWV4W0Y "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>


<a name="soccer_robot_design_infrared"></a>
## Soccer robot design(Infrared)
<img src="/assets/infrared_circuit.png" width="450">

[![Infrared test](https://img.youtube.com/vi/ilSl8ReIZsA/hqdefault.jpg)](https://youtu.be/ilSl8ReIZsA "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

<a name="soccer_robot_design_teleoperation"></a>
## Soccer robot design(Teleoperation)
It makes sense that the default motions performed in Gazebo simulations can be done in a real robot also. In order to confirm tat, the motion control function of  original version of Jetbot using the gamepad is slightly modified for Jetbot soccer version.

<img src="/assets/teleoperation_1.png" width="600">

You probably remember the gamepad UI that is in orginal teleoperation.ipynb file. Only 0, 1 scroolbal is used for controlling the left and right wheel in that file. In my case, I set it as above for controlling roller, stick, omniwheel.

```
from dynamixel_sdk import *                    # Uses Dynamixel SDK library

def omniwheel_move(motor_id, speed):
    # Control table address
    ADDR_MX_TORQUE_ENABLE      = 24               # Control table address is different in Dynamixel model
    ADDR_MX_GOAL_POSITION      = 30
    ADDR_MX_PRESENT_POSITION   = 36
    ADDR_MX_MOVING_SPEED       = 32

    # Protocol version
    PROTOCOL_VERSION            = 1.0               # See which protocol version is used in the Dynamixel
    
    # Default setting
    DXL_ID = motor_id
    #DXL_ID                      = 1                # Dynamixel ID : 1
    BAUDRATE                    = 57600             # Dynamixel default baudrate : 57600
    DEVICENAME                  = '/dev/ttyUSB0'    # Check which port is being used on your controller
                                                    # ex) Windows: "COM1"   Linux: "/dev/ttyUSB0" Mac: "/dev/tty.usbserial-*"

    TORQUE_ENABLE               = 1                 # Value for enabling the torque
    TORQUE_DISABLE              = 0                 # Value for disabling the torque
    DXL_MINIMUM_POSITION_VALUE  = 10           # Dynamixel will rotate between this value
    DXL_MAXIMUM_POSITION_VALUE  = 4000            # and this value (note that the Dynamixel would not move when the position value is out of movable range. Check e-manual about the range of the Dynamixel you use.)
    DXL_MOVING_STATUS_THRESHOLD = 20                # Dynamixel moving status threshold

    index = 0
    dxl_goal_position = [DXL_MINIMUM_POSITION_VALUE, DXL_MAXIMUM_POSITION_VALUE]         # Goal position

    # Initialize PortHandler instance
    # Set the port path
    # Get methods and members of PortHandlerLinux or PortHandlerWindows
    portHandler = PortHandler(DEVICENAME)

    # Initialize PacketHandler instance
    # Set the protocol version
    # Get methods and members of Protocol1PacketHandler or Protocol2PacketHandler
    packetHandler = PacketHandler(PROTOCOL_VERSION)

    # Open port
    if portHandler.openPort():
        #print("Succeeded to open the port")
        pass
    else:
        print("Failed to open the port")
        print("Press any key to terminate...")
        getch()
        quit()

    # Set port baudrate
    if portHandler.setBaudRate(BAUDRATE):
        #print("Succeeded to change the baudrate")
        pass
    else:
        print("Failed to change the baudrate")
        print("Press any key to terminate...")
        getch()
        quit()

    # Enable Dynamixel Torque
    #dxl_comm_result, dxl_error = packetHandler.write1ByteTxRx(portHandler, DXL_ID, ADDR_MX_TORQUE_ENABLE, TORQUE_ENABLE)
    dxl_comm_result, dxl_error = packetHandler.write2ByteTxRx(portHandler, DXL_ID, ADDR_MX_MOVING_SPEED, speed)
    #print("dxl_comm_result: " + str(dxl_comm_result))
    if dxl_comm_result != COMM_SUCCESS:
        print("%s" % packetHandler.getTxRxResult(dxl_comm_result))
    elif dxl_error != 0:
        print("%s" % packetHandler.getRxPacketError(dxl_error))
    else:
        pass
        #print("Dynamixel has been successfully connected")
```
```
# Stop
def stop():
    omniwheel_move(1,0)
    omniwheel_move(2,0)
    omniwheel_move(3,0)
    omniwheel_move(4,0)
```
```
import traitlets
from traitlets.config.configurable import Configurable

class OmniWheelStop(Configurable):
    value = traitlets.Float()

    def __init__(self, *args, **kwargs):
        super(OmniWheelStop, self).__init__(*args, **kwargs)  # initializes traitlets
    
    @traitlets.observe('value')
    def _observe_value(self, change):
        self._write_value(change['new'])

    def _write_value(self, value):
        """Sets motor value between [-1, 1]"""
        stop()

    def _release(self):
        """Stops motor by releasing control"""
        print("_release")
```
```
from jetbot import Robot
import traitlets

robot = Robot()

roller_link = traitlets.dlink((controller.axes[1], 'value'), (robot.right_motor, 'value'), transform=lambda x: -x)

stop_link = traitlets.dlink((controller.buttons[5], 'value'), (omniwheel_stop, 'value'), transform=lambda x: -x)
forward_link = traitlets.dlink((controller.buttons[0], 'value'), (omniwheel_forward, 'value'), transform=lambda x: -x)
left_link = traitlets.dlink((controller.buttons[1], 'value'), (omniwheel_left, 'value'), transform=lambda x: -x)
right_link = traitlets.dlink((controller.buttons[2], 'value'), (omniwheel_right, 'value'), transform=lambda x: -x)
back_link = traitlets.dlink((controller.buttons[3], 'value'), (omniwheel_back, 'value'), transform=lambda x: -x)
```

Like the original method, using the gamepad requires combining Traitlets package of python and control code of hardware. Above code is a sample for I use for omniwheel control. First, Dynamixel SDK is used for creating function that gives direction and speed to each omniwheel, and then use these function to create function for the robot to move forward, turn left, turn right, reverse, and stop. Then, using this function, a configurable class of traitlets is created, and the class declared at the end is connected to the gamepad UI using the dlink function of Traitlets.

[![Teleoperation test 1](https://img.youtube.com/vi/vONoIruznlw/hqdefault.jpg)](https://www.youtube.com/watch?v=vONoIruznlw "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

As you can see in the video, you can see the basic movement and ball dribbling are working well. However, kicking power is weak. It is considered that  voltage , current are insufficient because battery and solenoid are directly connected. To solve these problem, I decide to use the method of collecting the high voltage using a large capacitor and then releasing it to the solenoid at once.

[![Teleoperation test 2](https://img.youtube.com/vi/xpfglVhzQOg/hqdefault.jpg)](https://youtu.be/xpfglVhzQOg "Jetbot Soccer Play - Click to Watch!")
<strong>Click to Watch!</strong>

I test again basic soccer function after adding a large capacitor. It seems like that the kick function is reinforced and it can kick a balls to a long distance than before.

